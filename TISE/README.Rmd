---
output: github_document
---

<!-- TISE.md is generated from TISE.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```


# Tidy Data Principles for Intro Stats & Data Science Courses: The `fivethirtyeight` R Package

Notes:

* This is an outline only at this point
* References in parentheses


## Introduction

* **Who we are**: We are involved in statistics and data science education, in particular at the introductory undergraduate level.
* **Our Context and Background**: 
    + Albert: In math dept teaching service course
    + Chester: Instructional technologist + Teaching sociology majors
    + Jennifer: Business/economics students


## Motivation

We want to teach introductory statistics and data science courses that

1. **Have more computing** (Nolan, Gould)
1. **Leverage open source tools**
    1. Liberate students from computer lab with proprietary software to laptop (allows for more flexibility, blurs lines between lab & lecture)
    1. Open Data: MIT licence
1. **Are data-centric**: Not just any data, data that satisfy the 3 R's
    1. *Rich enough* to answer meaningful questions with
    1. *Real enough* to ensure that there is context
    1. *Realistic enough* to convey to students that data as it exists "in the wild" often needs processing (Jenny Bryan)
1. **Overarching goal**: Minimize prerequisites to research
    1. Phrase coined by Cobb in TAS
    1. Grolemund/Wickham Data/Science pipeline flow chart
    1. Intro Data Science I course from PCMI Guidelines

**Previous work**: R packages consisting chiefly of data sets have been very successful to this end. Much as the `iris`, `UCBAdmissions`, Old `faithful`, and `anscombe`'s quartet data sets have been canonical data sets in statistics, some new data sets are emerging to be canonical data sets in the era of Big Data pedagogy: `nycflights13` (wickham, baumer and horton), `babynames`, `Gapminder`, `okcupiddata` (JSE). 

It is along these lines that we present *`fivethirtyeight`: an R package of data and code behind the stories and interactives at [FiveThirtyEight.com](http://www.FiveThirtyEight.com)*.


## The Package

**About 538**: It is a data-driven journalism website founded by Nate Silver and owned by ESPN. FiveThirtyEight has been very forward thinking in making the data used in many of their articles open and accessible on GitHub, a web-based repository for collaboration on code and data. These data sets satisfy the 3 R's. 

**What we did**: All with the novice student in mind, we

1. Performed just enough pre-processing so that statistics and data science novices can sink their teeth into the data right away. We discuss the specifics in The Proposal chapter below.
1. As much as possible, have data sets fit into tidytools ecosystem.
1. Provided easily accessible documentation: The help file for each data set includes 
    1. a thorough description of the observational unit and all variables
    1. a link to the original article
    1. (if listed) the data sources
1. Include all source code to go from raw data to pre-processed data on GitHub page for package.
1. Packaged it all in an easy to load format
    1. lazyload instead of loading CSV files
    1. Documentation in help pages instead of README files
1. Started to crowdsource vignettes of data analysis examples of specific data sets. Ex: `bechdel`


## The Proposal

**PUNCHLINE**: After creating the `fivethirtyeight` package, and looking at the `nycflights13`, `babynames`, etc data sets, we feel the four principles below should be explicitly codified as *"tidy data principles for intro stats and data science courses"*, as an extension of *"tidy data principles"* espoused by (wickham tidy data paper, tidytools manifesto). 

1. These four principles align directly with the biggest challenges impeding the progress of R newbies, thereby stalling their ability to "look at data quick".
1. They all entail doing *just enough* pre-processing of data for students; think of mother robin regurgitating worms for young.
1. These principles also act as an unofficial set of standards, thereby
    1. Allowing for consistency in formats between data sets
    1. Seamless integration into the tidyverse/tidytools ecosystem
    1. Minimizing prerequisites for research

**PRINCIPLES**:

1. **Variable naming conventions** (R packages book on naming variables)
    1. Rule of thumb: no more than 20 characters max (if possible)
    1. All lower case
    1. Use underscores instead of CamelCase or spaces (necessitating `` marks when using tidytools)
1. **Dates**: Convert date variables that are beyond just year to POSIX date objects using the `lubridate` package. That way users can easily create time series plots. Example:
    1. If only a `year` variable exists, then we leave it as is.
    1. If there are `year` and `month` variables, we convert them to POSIX date objects as `year-month-01`.
    1. If there are `year`, `month`, and `day` variables, we convert them to POSIX date objects as `year-month-day`.
1. **Factors vs characters**:
    1. Ordinal categorical variables are `factor` with the intuitive ordering of levels. We did this to ensure barplots and boxplots would display in an intuitive order (McNamara/Horton) 
    1. Regular categorical variables are left as `character` vectors.
1. **Tidy data format**: Provide data in tidy data format (or at the least code to convert)
    1. Whenever possible, save data in tidy data format. Ex: for `comic_characters`, originally the data sets were "Marvel" and "DC" comics separately. We combined with new variable `publisher`
    1. If converting to tidy format would alter the data set from the originaly, then at the very least provide `tidyr::gather()` code in `@examples` section of help files.




## The Future

1. Immediate Applications:
    1. With minimal disruption to syllabi of R-focused courses, increase data-centrism by emphasizing context of data. How? Make them read the articles!
    1. Standardization: data sets like 538 and any that fit our proposed model play nice with `tidyverse` ecosystem: `dplyr`, `ggplot2`, `broom`, `modelr`, etc
1. Medium-Term Applications: Increase computational content of intro courses
    1. Incorporate into ModernDive: An Introduction to Statistical and Data Sciences, a book build on `bookdown` <http://www.moderndive.com/>
    1. DataCamp: Chester to expand on this: <https://www.datacamp.com/courses/effective-data-storytelling-using-the-tidyverse>
1. Long-Term Vision: All this is a small part of a potential new paradigm. **Data analysis as an alternative vehicle/medium to introduce algorithmic thinking, computational logic, and coding/programming skills to novices instead of the traditional CS approach of data structures (linked lists, stacks, hash tables) and algorithms (recursion, sorting, searching)**
    + Example: Section 2.2.5 of Data Science in Statistics Curricula: Preparing Students to "Think with Data"
    + Potential benefits:
        + (Anecdotal so far) Appeals to a broader base of students, in particular those from underrepresented groups.
        + Gets more people coding. Less and less a vocational skill, but more and more of a basic skill like reading, writing, arithmetic. Obama's video.
